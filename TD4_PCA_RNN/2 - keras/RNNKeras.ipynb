{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNNKeras.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "czknBR-X_YFr"
      },
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Dropout, Activation\n",
        "from keras.optimizers import RMSprop, Adam\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOXnLv5E_cHV"
      },
      "source": [
        "SEQ_LENGTH = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6jL-pPQ_fOh"
      },
      "source": [
        "def buildmodel(VOCABULARY):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(256, input_shape = (SEQ_LENGTH, 1), return_sequences = True))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(LSTM(256))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(VOCABULARY, activation = 'softmax'))\n",
        "    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHmG5CMJ_gT2"
      },
      "source": [
        "file = open('randomText.txt', encoding = 'utf8')\n",
        "raw_text = file.read()    #you need to read further characters as well\n",
        "raw_text = raw_text.lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6MvuRix9_g4t",
        "outputId": "9b590cad-c6f3-46a8-e57c-caec6bd78c12"
      },
      "source": [
        "chars = sorted(list(set(raw_text)))\n",
        "print(chars)\n",
        "\n",
        "bad_chars = ['#', '*', '@', '_', '\\ufeff']\n",
        "for i in range(len(bad_chars)):\n",
        "    raw_text = raw_text.replace(bad_chars[i],\"\")\n",
        "\n",
        "chars = sorted(list(set(raw_text)))\n",
        "print(chars)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['\\n', ' ', \"'\", ',', '-', '.', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'x', 'y', 'z', 'à', 'â', 'è', 'é', 'ê', 'î', 'ï', 'ô', 'û']\n",
            "['\\n', ' ', \"'\", ',', '-', '.', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'x', 'y', 'z', 'à', 'â', 'è', 'é', 'ê', 'î', 'ï', 'ô', 'û']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zht9_fh_m8l",
        "outputId": "dee4db64-bea0-4e19-f09e-badcee0c79b6"
      },
      "source": [
        "text_length = len(raw_text)\n",
        "char_length = len(chars)\n",
        "VOCABULARY = char_length\n",
        "print(\"Text length = \" + str(text_length))\n",
        "print(\"No. of characters = \" + str(char_length))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text length = 860\n",
            "No. of characters = 39\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-ZBRJL7_pUZ"
      },
      "source": [
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "input_strings = []\n",
        "output_strings = []\n",
        "\n",
        "for i in range(len(raw_text) - SEQ_LENGTH):\n",
        "    X_text = raw_text[i: i + SEQ_LENGTH]\n",
        "    X = [char_to_int[char] for char in X_text]\n",
        "    input_strings.append(X)    \n",
        "    Y = raw_text[i + SEQ_LENGTH]\n",
        "    output_strings.append(char_to_int[Y])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MctcaZ8i_ryN",
        "outputId": "c52f6df2-b220-4426-d0ea-ceb6c1220c30"
      },
      "source": [
        "length = len(input_strings)\n",
        "input_strings = np.array(input_strings)\n",
        "input_strings = np.reshape(input_strings, (input_strings.shape[0], input_strings.shape[1], 1))\n",
        "input_strings = input_strings/float(VOCABULARY)\n",
        "\n",
        "output_strings = np.array(output_strings)\n",
        "output_strings = np_utils.to_categorical(output_strings)\n",
        "print(input_strings.shape)\n",
        "print(output_strings.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(760, 100, 1)\n",
            "(760, 39)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TA1C2YcI_u9e",
        "outputId": "1b7b33b9-9cf3-4c54-ecaf-b9e48ef48800"
      },
      "source": [
        "model = buildmodel(VOCABULARY)\n",
        "filepath=\"saved_models/weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "history = model.fit(input_strings, output_strings, epochs = 30, batch_size = 128, callbacks = callbacks_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "6/6 [==============================] - ETA: 0s - loss: 3.4780\n",
            "Epoch 00001: loss improved from inf to 3.47802, saving model to saved_models/weights-improvement-01-3.4780.hdf5\n",
            "6/6 [==============================] - 8s 1s/step - loss: 3.4780\n",
            "Epoch 2/30\n",
            "6/6 [==============================] - ETA: 0s - loss: 3.1039\n",
            "Epoch 00002: loss improved from 3.47802 to 3.10390, saving model to saved_models/weights-improvement-02-3.1039.hdf5\n",
            "6/6 [==============================] - 8s 1s/step - loss: 3.1039\n",
            "Epoch 3/30\n",
            "6/6 [==============================] - ETA: 0s - loss: 3.0258\n",
            "Epoch 00003: loss improved from 3.10390 to 3.02578, saving model to saved_models/weights-improvement-03-3.0258.hdf5\n",
            "6/6 [==============================] - 8s 1s/step - loss: 3.0258\n",
            "Epoch 4/30\n",
            "6/6 [==============================] - ETA: 0s - loss: 3.0269\n",
            "Epoch 00004: loss did not improve from 3.02578\n",
            "6/6 [==============================] - 9s 1s/step - loss: 3.0269\n",
            "Epoch 5/30\n",
            "6/6 [==============================] - ETA: 0s - loss: 3.0252\n",
            "Epoch 00005: loss improved from 3.02578 to 3.02522, saving model to saved_models/weights-improvement-05-3.0252.hdf5\n",
            "6/6 [==============================] - 8s 1s/step - loss: 3.0252\n",
            "Epoch 6/30\n",
            "6/6 [==============================] - ETA: 0s - loss: 2.9999\n",
            "Epoch 00006: loss improved from 3.02522 to 2.99988, saving model to saved_models/weights-improvement-06-2.9999.hdf5\n",
            "6/6 [==============================] - 8s 1s/step - loss: 2.9999\n",
            "Epoch 7/30\n",
            "6/6 [==============================] - ETA: 0s - loss: 3.0141\n",
            "Epoch 00007: loss did not improve from 2.99988\n",
            "6/6 [==============================] - 9s 1s/step - loss: 3.0141\n",
            "Epoch 8/30\n",
            "6/6 [==============================] - ETA: 0s - loss: 3.0081\n",
            "Epoch 00008: loss did not improve from 2.99988\n",
            "6/6 [==============================] - 8s 1s/step - loss: 3.0081\n",
            "Epoch 9/30\n",
            "6/6 [==============================] - ETA: 0s - loss: 3.0145\n",
            "Epoch 00009: loss did not improve from 2.99988\n",
            "6/6 [==============================] - 8s 1s/step - loss: 3.0145\n",
            "Epoch 10/30\n",
            "6/6 [==============================] - ETA: 0s - loss: 3.0053\n",
            "Epoch 00010: loss did not improve from 2.99988\n",
            "6/6 [==============================] - 8s 1s/step - loss: 3.0053\n",
            "Epoch 11/30\n",
            "6/6 [==============================] - ETA: 0s - loss: 3.0054\n",
            "Epoch 00011: loss did not improve from 2.99988\n",
            "6/6 [==============================] - 8s 1s/step - loss: 3.0054\n",
            "Epoch 12/30\n",
            "6/6 [==============================] - ETA: 0s - loss: 2.9956\n",
            "Epoch 00012: loss improved from 2.99988 to 2.99555, saving model to saved_models/weights-improvement-12-2.9956.hdf5\n",
            "6/6 [==============================] - 8s 1s/step - loss: 2.9956\n",
            "Epoch 13/30\n",
            "6/6 [==============================] - ETA: 0s - loss: 2.9939\n",
            "Epoch 00013: loss improved from 2.99555 to 2.99386, saving model to saved_models/weights-improvement-13-2.9939.hdf5\n",
            "6/6 [==============================] - 8s 1s/step - loss: 2.9939\n",
            "Epoch 14/30\n",
            "6/6 [==============================] - ETA: 0s - loss: 3.0114\n",
            "Epoch 00014: loss did not improve from 2.99386\n",
            "6/6 [==============================] - 8s 1s/step - loss: 3.0114\n",
            "Epoch 15/30\n",
            "6/6 [==============================] - ETA: 0s - loss: 3.0100\n",
            "Epoch 00015: loss did not improve from 2.99386\n",
            "6/6 [==============================] - 8s 1s/step - loss: 3.0100\n",
            "Epoch 16/30\n",
            "6/6 [==============================] - ETA: 0s - loss: 3.0007\n",
            "Epoch 00016: loss did not improve from 2.99386\n",
            "6/6 [==============================] - 8s 1s/step - loss: 3.0007\n",
            "Epoch 17/30\n",
            "6/6 [==============================] - ETA: 0s - loss: 2.9947\n",
            "Epoch 00017: loss did not improve from 2.99386\n",
            "6/6 [==============================] - 9s 1s/step - loss: 2.9947\n",
            "Epoch 18/30\n",
            "6/6 [==============================] - ETA: 0s - loss: 2.9940\n",
            "Epoch 00018: loss did not improve from 2.99386\n",
            "6/6 [==============================] - 9s 1s/step - loss: 2.9940\n",
            "Epoch 19/30\n",
            "6/6 [==============================] - ETA: 0s - loss: 2.9915\n",
            "Epoch 00019: loss improved from 2.99386 to 2.99152, saving model to saved_models/weights-improvement-19-2.9915.hdf5\n",
            "6/6 [==============================] - 8s 1s/step - loss: 2.9915\n",
            "Epoch 20/30\n",
            "6/6 [==============================] - ETA: 0s - loss: 3.0045\n",
            "Epoch 00020: loss did not improve from 2.99152\n",
            "6/6 [==============================] - 8s 1s/step - loss: 3.0045\n",
            "Epoch 21/30\n",
            "6/6 [==============================] - ETA: 0s - loss: 2.9960\n",
            "Epoch 00021: loss did not improve from 2.99152\n",
            "6/6 [==============================] - 8s 1s/step - loss: 2.9960\n",
            "Epoch 22/30\n",
            "6/6 [==============================] - ETA: 0s - loss: 2.9890\n",
            "Epoch 00022: loss improved from 2.99152 to 2.98903, saving model to saved_models/weights-improvement-22-2.9890.hdf5\n",
            "6/6 [==============================] - 8s 1s/step - loss: 2.9890\n",
            "Epoch 23/30\n",
            "6/6 [==============================] - ETA: 0s - loss: 2.9840\n",
            "Epoch 00023: loss improved from 2.98903 to 2.98400, saving model to saved_models/weights-improvement-23-2.9840.hdf5\n",
            "6/6 [==============================] - 8s 1s/step - loss: 2.9840\n",
            "Epoch 24/30\n",
            "6/6 [==============================] - ETA: 0s - loss: 2.9902\n",
            "Epoch 00024: loss did not improve from 2.98400\n",
            "6/6 [==============================] - 8s 1s/step - loss: 2.9902\n",
            "Epoch 25/30\n",
            "6/6 [==============================] - ETA: 0s - loss: 3.0005\n",
            "Epoch 00025: loss did not improve from 2.98400\n",
            "6/6 [==============================] - 8s 1s/step - loss: 3.0005\n",
            "Epoch 26/30\n",
            "6/6 [==============================] - ETA: 0s - loss: 2.9939\n",
            "Epoch 00026: loss did not improve from 2.98400\n",
            "6/6 [==============================] - 8s 1s/step - loss: 2.9939\n",
            "Epoch 27/30\n",
            "6/6 [==============================] - ETA: 0s - loss: 2.9849\n",
            "Epoch 00027: loss did not improve from 2.98400\n",
            "6/6 [==============================] - 8s 1s/step - loss: 2.9849\n",
            "Epoch 28/30\n",
            "6/6 [==============================] - ETA: 0s - loss: 2.9926\n",
            "Epoch 00028: loss did not improve from 2.98400\n",
            "6/6 [==============================] - 8s 1s/step - loss: 2.9926\n",
            "Epoch 29/30\n",
            "6/6 [==============================] - ETA: 0s - loss: 2.9877\n",
            "Epoch 00029: loss did not improve from 2.98400\n",
            "6/6 [==============================] - 8s 1s/step - loss: 2.9877\n",
            "Epoch 30/30\n",
            "6/6 [==============================] - ETA: 0s - loss: 2.9858\n",
            "Epoch 00030: loss did not improve from 2.98400\n",
            "6/6 [==============================] - 8s 1s/step - loss: 2.9858\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChfE1JPsHjxe"
      },
      "source": [
        "model.save(\"/content/drive/MyDrive/ING3/NLP/TD4_PCA/weights-improvement-49-2.8837.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBQ4LvwXAL8m"
      },
      "source": [
        "Text generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rvig1fQAFRA"
      },
      "source": [
        "filename = '/content/drive/MyDrive/ING3/NLP/TD4_PCA/weights-improvement-49-2.8837.hdf5'\n",
        "model = buildmodel(VOCABULARY)\n",
        "model.load_weights(filename)\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tYoVsdfAPET"
      },
      "source": [
        "initial_text = \"héroïque mais inutile, me fit beaucoup de questions pour savoir comment il avait tué. voudriez-vous \"\n",
        "initial_text = [char_to_int[c] for c in initial_text]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "587OhTlMARge",
        "outputId": "e6099697-799e-4f3e-92d0-a7a0d68226ce"
      },
      "source": [
        "GENERATED_LENGTH = 1000\n",
        "test_text = initial_text\n",
        "generated_text = []\n",
        "\n",
        "for i in range(1000):\n",
        "    X = np.reshape(test_text, (1, SEQ_LENGTH, 1))\n",
        "    next_character = model.predict(X/float(VOCABULARY))\n",
        "    index = np.argmax(next_character)\n",
        "    generated_text.append(chars[index])\n",
        "    test_text.append(index)\n",
        "    test_text = test_text[1:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n",
            "  1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oc1EQG5yATpw",
        "outputId": "2a12fd07-8cee-4cf2-e9c4-dfed0bfc09df"
      },
      "source": [
        "print(''.join(generated_text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjM1wXfJAX0q"
      },
      "source": [
        "Conclusion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XON5_r22AXZO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "7ff1b5c2-33e5-48af-d86b-8da575667553"
      },
      "source": [
        "original_text = []\n",
        "predicted_text = []\n",
        "\n",
        "text = widgets.Text()\n",
        "display(text)\n",
        "\n",
        "def handle_submit(sender):\n",
        "    global predicted_text\n",
        "    global original_text\n",
        "    \n",
        "    inp = list(text.value)\n",
        "    \n",
        "    last_word = inp[len(original_text):]\n",
        "    inp = inp[:len(original_text)]    \n",
        "    original_text = text.value    \n",
        "    last_word.append(' ')\n",
        "    \n",
        "    inp_text = [char_to_int[c] for c in inp]\n",
        "    last_word = [char_to_int[c] for c in last_word]\n",
        "    \n",
        "    if len(inp_text) > 100:\n",
        "        inp_text = inp_text[len(inp_text)-100: ]\n",
        "    if len(inp_text) < 100:\n",
        "        pad = []\n",
        "        space = char_to_int[' ']\n",
        "        pad = [space for i in range(100-len(inp_text))]\n",
        "        inp_text = pad + inp_text\n",
        "    \n",
        "    while len(last_word)>0:\n",
        "        X = np.reshape(inp_text, (1, SEQ_LENGTH, 1))\n",
        "        next_char = model.predict(X/float(VOCABULARY))\n",
        "        inp_text.append(last_word[0])\n",
        "        inp_text = inp_text[1:]\n",
        "        last_word.pop(0)\n",
        "    \n",
        "    next_word = []\n",
        "    next_char = ':'\n",
        "    while next_char != ' ':\n",
        "        X = np.reshape(inp_text, (1, SEQ_LENGTH, 1))\n",
        "        next_char = model.predict(X/float(VOCABULARY))\n",
        "        index = np.argmax(next_char)        \n",
        "        next_word.append(int_to_char[index])\n",
        "        inp_text.append(index)\n",
        "        inp_text = inp_text[1:]\n",
        "        next_char = int_to_char[index]\n",
        "    \n",
        "    predicted_text = predicted_text + [''.join(next_word)]\n",
        "    print(\" \" + ''.join(next_word), end='|')\n",
        "    \n",
        "text.on_submit(handle_submit)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-c7bec539c05a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpredicted_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwidgets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'widgets' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYbe9S5gAbdW"
      },
      "source": [
        "from tabulate import tabulate\n",
        "\n",
        "original_text = original_text.split()\n",
        "predicted_text.insert(0,\"\")\n",
        "predicted_text.pop()\n",
        "\n",
        "table = []\n",
        "for i in range(len(original_text)):\n",
        "    table.append([original_text[i], predicted_text[i]])\n",
        "print(tabulate(table, headers = ['Actual Word', 'Predicted Word']))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}